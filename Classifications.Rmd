---
title: "Clasificarea potabilității apei"
output:
  html_document:
    df_print: paged
---


- Încărcarea bibliotecilor necesare în proiect:

```{r results = 'hide'}
library(tidyverse)
library(modelr)
library(caret)
library(rsample)
library(corrplot)
library(dplyr)
library(magrittr)
library(corrplot)
library(rpart)
library(rpart.plot)
library(ISLR)
library(partykit)
library(tree)
library(ipred)
library(randomForest)
library(ranger)
library(pROC)
library(ggplot2)
library(reshape2)
```



- Încărcarea bazei de date:
```{r}
water <- read.csv("water_quality.csv")
data(water)
summary(water)
```



- Curățarea datelor

Convertim atributul ammonia din caracter in numeric (chr to dbl).
```{r}
water$ammonia <- as.numeric(water$ammonia)
```

Eliminăm eventualele observații care conțin valori lipsă si redenumim variabila dependenta.
```{r}
(water <- drop_na(water) %>%
  rename(potability = is_safe))
```



Transformăm atributul potability in factor:
```{r}
water <- water %>% 
  mutate(potability = ifelse(potability == 0, "No", "Yes"))
water <- water %>% mutate(potability = factor(potability))
table(water$potability)
```


_______________________________________________________________________________

## Vizualizarea datelor


```{r}
water %>% ggplot(aes(x=silver,y=perchlorate,color=potability))+
  geom_point()
water %>% ggplot(aes(x=aluminium,y=cadmium,color=potability))+
  geom_point()
water %>% ggplot(aes(x=aluminium,y=barium,color=potability))+
  geom_point()
```

Grupam datele dupa variabila tinta potabilitate. Observam ca avem mai multe date pentru valoarea No, deci mai multe date pentru apa nepotabila.
```{r}
by_potability <- group_by(water, potability)
summarize(by_potability, count=n())
```


- Vizualizarea valorilor medii ale fiecărei variabile, comparativ pentru apa potabilă și nepotabilă:    

```{r}

water %>% group_by(potability) %>%
  summarize_all(~mean(.)) %>%
  ungroup() %>%
  #restructuram dataset - in afara de potability, preluam toate col, names_to preia de pe coloane, values_to preia din celule
  pivot_longer(!potability, names_to = "features", values_to = "min") %>%
  #cream graficul pe orizontala prop, pe vert valori min)
  ggplot(aes(features, min, fill = potability)) +
  #design-ul graficului
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~features, scales = "free")
```
- Vizualizarea distribuției valorilor atributelor, pe categorii de potabilitate, sub forma de boxplot:  

```{r}
water.m <- melt(water, id.var = "potability")
p <- ggplot(data = water.m, aes(x=variable, y=value)) + 
             geom_boxplot(aes(fill=potability))
p + facet_wrap( ~ variable, scale="free")
```
  
- Distributia valorilor fiecărei variabile pentru apa nepotabilă (digrame de densitate):  

```{r}
water %>%
  filter(potability == "No") %>%
  select_if(is.numeric) %>%
  gather(metric, value) %>%
  ggplot(aes(value, fill=metric)) +
  geom_density(show.legend = FALSE) +
  facet_wrap(~metric, scales = "free")
```

- Matricea de corelație între proprietățile chimice:

```{r}
correlations <- cor(water[,1:20])
# create correlation plot
corrplot(correlations, method="circle")
```






## Împărțirea datelor 

Împărțim datele în set de antrenare și de test:
```{r}
set.seed(123)
split <- initial_split(water, prop=0.7, strata= "potability")
train <-training(split)
test <- testing(split)
```


Selectăm în features toate atributele în afară de potability:
```{r}
features <- setdiff(names(train), "potability")
features
```

Separăm variabilele
```{r}
#x sunt variabilele independente (predictori)
#y este variabila dependenta (potabilitatea)
x <- train[,features]
y <- train$potability
```


10-fold Cross-Validation
```{r}
train_control <- trainControl(
  method="cv",
  number=10) 
```


__________________________________________________________________________________________

## Regresie Logistică

Generarea câte unui model in funcție de proprietati:
```{r}
model_al<-glm(data=water, potability ~ aluminium, family=binomial)
summary(model_al)$coefficients 

model_am<-glm(data=water, potability ~ ammonia, family=binomial)
summary(model_am)$coefficients

model_ar<-glm(data=water, potability ~ arsenic, family=binomial)
summary(model_ar)$coefficients

model_br<-glm(data=water, potability ~ barium, family=binomial)
summary(model_br)$coefficients

model_cd<-glm(data=water, potability ~ cadmium, family=binomial)
summary(model_cd)$coefficients

model_cl<-glm(data=water, potability ~ chloramine, family=binomial)
summary(model_cl)$coefficients

model_cr<-glm(data=water, potability ~ chromium, family=binomial)
summary(model_cr)$coefficients

model_cp<-glm(data=water, potability ~ copper, family=binomial)
summary(model_cp)$coefficients

model_fl<-glm(data=water, potability ~ flouride, family=binomial)
summary(model_fl)$coefficients

model_bc<-glm(data=water, potability ~ bacteria, family=binomial)
summary(model_bc)$coefficients

model_vs<-glm(data=water, potability ~ viruses, family=binomial)
summary(model_vs)$coefficients

model_ld<-glm(data=water, potability ~ lead, family=binomial)
summary(model_ld)$coefficients

model_na<-glm(data=water, potability ~ nitrates, family=binomial)
summary(model_na)$coefficients

model_ni<-glm(data=water, potability ~ nitrites, family=binomial)
summary(model_ni)$coefficients

model_mc<-glm(data=water, potability ~ mercury, family=binomial)
summary(model_mc)$coefficients

model_pc<-glm(data=water, potability ~ perchlorate, family=binomial)
summary(model_pc)$coefficients

model_rd<-glm(data=water, potability ~ radium, family=binomial)
summary(model_rd)$coefficients

model_sl<-glm(data=water, potability ~ selenium, family=binomial)
summary(model_sl)$coefficients

model_sv<-glm(data=water, potability ~ silver, family=binomial)
summary(model_sv)$coefficients

model_ur<-glm(data=water, potability ~ uranium, family=binomial)
summary(model_ur)$coefficients  
```
Observând rezultatele obținute, am descoperit faptul ca variabilele sunt relevante, mai putin florul, care are o valoare mare a lui p. 


Crearea modelelor pe date specifice
1. Primul model creat pe toate variabilele 
```{r}
model_final <- glm(data=train, potability~aluminium+barium+chloramine+chromium+silver+copper+nitrites+perchlorate+radium+ammonia+arsenic+cadmium+bacteria+viruses+lead+nitrates+mercury+selenium+uranium+flouride, family=binomial)
summary(model_final)$coefficients

```
Observam faptul ca proprietatea aluminium este cea mai seminificativa, deoarece inregistreaza cea mai mica valoare a lui p. 


Realizăm matricea de confuzie, considerând valoarea de 0.3 drept prag pentru clasificarea in Yes/No:
```{r}
pred_mx <- predict(model_final, newdata=test)
(conf1 <- confusionMatrix(factor(ifelse(pred_mx>0.3, "Yes", "No")), factor(test$potability)))

```

2. Modelul 2 s-a realizat pe variabilele independente cu o relatie directa determinantă cu variabila dependenta:
```{r}
model_2 <- glm(data=train, potability~aluminium+barium+chloramine+chromium+copper, family=binomial)
summary(model_2)
```
```{r}
pred_test_2 <- predict(model_2, newdata=test, type="response")
(conf2 <- confusionMatrix(factor(ifelse(pred_test_2>0.3, "Yes", "No")), factor(test$potability)))
```


3. Modelul 3 s-a realizat pe variabilele independente cu o relatie indirecta cu variabila dependenta:
```{r}
model_3 <- glm(data=train, potability~arsenic+cadmium+mercury+selenium+uranium, family=binomial)
summary(model_3)
```

```{r}
pred_test_3 <- predict(model_3, newdata=test, type="response")
(conf3 <- confusionMatrix(factor(ifelse(pred_test_3>0.3, "Yes", "No")), factor(test$potability)))
```


4. Model folosind Cross-Validation:
```{r}
model_cv_lr  <- train (
  x=x,
  y=y,
  method="glm",
  family="binomial",
  trControl = train_control
)
model_cv_lr

```

```{r}
pred_cv_lr = predict (model_cv_lr, newdata=test, type="raw")
(conf4 <- confusionMatrix(pred_cv_lr, test$potability))
```


Tabel comparativ:  

```{r}
model <- c('model_final','model_2','model_3', 'model_cv_lr')
accuracy <- c(conf1$overall['Accuracy'], conf2$overall['Accuracy'], conf3$overall['Accuracy'], conf4$overall['Accuracy'])
sensitivity <- c(conf1$byClass['Sensitivity'], conf2$byClass['Sensitivity'], conf3$byClass['Sensitivity'], conf4$byClass['Sensitivity'])
specificity <- c(conf1$byClass['Specificity'], conf2$byClass['Specificity'], conf3$byClass['Specificity'], conf4$byClass['Specificity'])
performance <- data.frame(model, accuracy, sensitivity, specificity)
performance
```
In urma rezultatelor obtinute, alegem ca model reprezentativ modelul obtinut prin cross-validation (model_cv_lr), deoarece prezinta o acuratete ridicata, cât și o rată a specificității ridicată. Deși acuratețea primului model este mai ridicată, procentul nu e semnificativ, specificitatea e redusa, de aceea am ales ultimul model in detrimentului primului.

```{r}
best_lr <- model_cv_lr
```









________________________________________________________________________________

## Naive Bayes 






1. Primul model, realizat cu Cross-Validation:
```{r warning = FALSE}
mod_nb1 <- train(
  x = x,
  y = y,
  method = "nb",
  trControl = train_control )
mod_nb1
```



```{r}
confusionMatrix(mod_nb1)
```


Search grid pentru a identifica parametrii optimi:
```{r}
search_grid <- expand.grid(
  usekernel = c(TRUE, FALSE),
  fL = 0.5, #la variabilele nominale sa nu porneasca de la 0
  adjust = seq(0, 5, by = 1) ) #ajusteaza kernelii cu (1,2,3,4,5)
```

2. Model cu optimizarea parametrilor:

```{r warning = FALSE}
mod_nb2 = train(
  x = x,
  y = y,
  method = "nb",
  trControl = train_control,
  tuneGrid = search_grid     
)

mod_nb2
```


```{r}
confusionMatrix(mod_nb2)
```

Observăm o acuratețe net mai mare (90,83% vs 85,6%), deci alegem acest model.
```{r}
best_nb <- mod_nb2$finalModel
```



```{r warning = FALSE}
#predictie pe setul de test, cu probabilitatea aferenta
pred_nb <- predict(best_nb, test, type="prob")
#compara predictiile cu valorile reale
confusionMatrix(pred_nb$class, test$potability)
#95% CI = confidence intervals
#no information rate (NIR) e sansa de a face clasificarea random, fara un model
```









________________________________________________________________________________________

## Arbori de decizie 


1. Arbore simplu folosind pachetul rpart:
```{r}
set.seed(123)
mod_tree1 = rpart(
  formula = potability ~. ,  
  data = train,
  method = "class" 
)
mod_tree1
#nr instante, loss = nr erori (clasificate gresit pe clasa majoritara), clasa majoritara, probabilitatile predictiei 
```

```{r}
summary(mod_tree1)
#CP = cost complexity parameters
#penalizeaza arborele pe baza nr de frunze
```


```{r}
rpart.plot(mod_tree1)
```


```{r}
#predictie pe test
pred_mod_tree1 <- predict(mod_tree1, newdata = test, target ="class")
#adaugam coloana cu no/yes
pred_mod_tree1 <- as_tibble(pred_mod_tree1) %>% mutate(class = ifelse(No >= Yes, "No", "Yes"))
(cmt1 <- confusionMatrix(factor(pred_mod_tree1$class), factor(test$potability)))
```


Comparatie cu un arbore simplu realizat cu Cross-Validation:
```{r}
mod_tree_cv <- train(x,y,
                     method = "rpart",
                      trControl = train_control)
pred_mod_tree_cv <- predict(mod_tree_cv, newdata = test, target ="class")
(cmt_cv <- confusionMatrix(pred_mod_tree_cv, factor(test$potability)))
```
Modelul realizat prin rpart este cu putin mai bun.





2. Arbore netaiat
```{r}
mod_tree_full <- rpart(potability ~., 
            data = train,
            method = "class",
            control = list(cp=0)) 
rpart.plot(mod_tree_full)
```


Acuratete mai buna pentru arborele netaiat:
```{r}
pred_tree_full <- predict(mod_tree_full, newdata = test, target = "class")
pred_tree_full <- as_tibble(pred_tree_full) %>% mutate(class = ifelse(No >= Yes, "No", "Yes"))
(cmt2 <- confusionMatrix(factor(pred_tree_full$class), factor(test$potability)))
```


3. Pruning: 

```{r}
m2_pruned <- prune(mod_tree_full, cp = 0.005)
pred_m2_pruned <- predict(m2_pruned, newdata = test, target = "class")
pred_m2_pruned <- as_tibble(pred_m2_pruned) %>% mutate(class = ifelse(No >= Yes, "No", "Yes"))
(cmt3 <- confusionMatrix(factor(pred_m2_pruned$class), factor(test$potability)))
```



4. Cu optimizarea entropiei, folosind functia tree():
```{r}
set.seed(123)
mod_tree_entropy <- tree(potability ~., data = train) 
summary(mod_tree_entropy)
```


```{r}
pred_tree_entropy <- predict(mod_tree_entropy, newdata = test, target = "class")
pred_tree_entropy <- as_tibble(pred_tree_entropy) %>% mutate(class = ifelse(No >= Yes, "No", "Yes"))
(cmt4 <- confusionMatrix(factor(pred_tree_entropy$class), factor(test$potability)))
```


5. Cu optimizarea coeficientului Gini

```{r}
set.seed(123)
mod_tree_gini <- tree(potability ~., data = train, split="gini")
summary(mod_tree_gini)
```


```{r}
pred_tree_gini <- predict(mod_tree_gini, newdata = test, target = "class")
pred_tree_gini <- as_tibble(pred_tree_gini) %>% mutate(class = ifelse(No >= Yes, "No", "Yes"))
(cmt5 <- confusionMatrix(factor(pred_tree_gini$class), factor(test$potability)))
```




Evaluarea performantei arborilor de decizie:


```{r}
model <- c('tree1','tree_full','tree_pruned','tree_entropy','tree_gini')
accuracy <- c(cmt1$overall['Accuracy'], cmt2$overall['Accuracy'], cmt3$overall['Accuracy'],cmt4$overall['Accuracy'],cmt5$overall['Accuracy'])
sensitivity <- c(cmt1$byClass['Sensitivity'], cmt2$byClass['Sensitivity'], cmt3$byClass['Sensitivity'],cmt4$byClass['Sensitivity'],cmt5$byClass['Sensitivity'])
specificity <- c(cmt1$byClass['Specificity'], cmt2$byClass['Specificity'], cmt3$byClass['Specificity'],cmt4$byClass['Specificity'],cmt4$byClass['Specificity'])
performance <- data.frame(model, accuracy, sensitivity, specificity)
performance
```

În acest caz, desi arborele netaiat pare cel mai perfomant, vom face exceptia de alege următorul model ca performanță (tree_gini), pentru a evita overfitting-ul.

```{r}
best_tree <- mod_tree_gini
```



__________________________________________________________________________________________

## Bagging


1. Model simplu:
```{r}
set.seed(123)
mod_bagged1 <- bagging(potability ~ .,
                     data = train, coob = TRUE)
mod_bagged1
```
Acuratetea de training = 100 - 3.82 = 96.18%


```{r}
pred_mod_bagged1 <- predict(mod_bagged1, newdata = test, target = "class")
(cmb1 <- confusionMatrix(pred_mod_bagged1, factor(test$potability)))
```


Grafic pentru stabilizarea erorii de clasificare:
```{r}
ntree <- seq(10, 50, by = 1)
misclassification <- vector(mode = "numeric", length = length(ntree))
for (i in seq_along(ntree)) {
  set.seed(123)
  model <- bagging( 
    potability ~.,
    data = train,
    coob = TRUE,
    nbag = ntree[i])
  misclassification[i] = model$err
}

plot(ntree, misclassification, type="l", lwd="2")
```

2. Model cu optimizarea numarului de bags:
Optăm pentru 36 de bags:

```{r}
mod_bagged_36 <- bagging(potability ~ .,
                     data = train, coob = TRUE, nbag = 36)
mod_bagged_36
```

```{r}
pred_mod_bagged_36 <- predict(mod_bagged_36, newdata = test, target = "class")
(cmb2 <- confusionMatrix(pred_mod_bagged_36, factor(test$potability)))
```



3. Model Bagging cu caret si Cross-Validation:

```{r}
mod_bagged_cv <- train(x,
                   y,
                   method="treebag",
                   trControl=train_control,
                   nbag = 36,
                   importance=TRUE)

mod_bagged_cv
```


```{r}
pred_mod_bagged_cv <- predict(mod_bagged_cv, newdata = test, target = "class")
(cmb3 <- confusionMatrix(pred_mod_bagged_cv, factor(test$potability)))
```



```{r}
model <- c('bagged1','bagged2','bagged_cv')
accuracy <- c(cmb1$overall['Accuracy'], cmb2$overall['Accuracy'], cmb3$overall['Accuracy'])
sensitivity <- c(cmb1$byClass['Sensitivity'], cmb2$byClass['Sensitivity'], cmb3$byClass['Sensitivity'])
specificity <- c(cmb1$byClass['Specificity'], cmb2$byClass['Specificity'], cmb3$byClass['Specificity'])
performance <- data.frame(model, accuracy, sensitivity, specificity)
performance
```

Vom selecta modelul bagged2 drept cel mai performant:

```{r}
best_bagged <- mod_bagged_36
```



_____________________________________________________________________________________

## Random Forest 



1. Model simplu cu biblioteca randomForest:
```{r}
set.seed(123)
mod_rf1 <- randomForest(
  formula = potability ~ .,
  data = train
) #selecteaza aleatoriu 6 variabile la fiecare impartire 
#pt ca avem 20 variabile in total si m=p/3 =11/3 aprox = 3
mod_rf1
```


Vizualizarea evolutiei ratei erorii de clasificare:
```{r}
plot(mod_rf1)
```


```{r}
pred_mod_rf1 <- predict(mod_rf1, test, target ="class")  
(cm1 <- confusionMatrix(pred_mod_rf1, factor(test$potability)))
```


Tuning:

```{r}
set.seed(123)
hyper_grid <- expand.grid(
  mtry = seq(2, 20, by = 1), # nr de atribute dintr-un split;
  node_size = seq(3, 9, by = 2), 
  sample_size = c(.55, .632, .7, .8),
  OOB_ERR = 0 
)


for (i in 1:nrow(hyper_grid)) {
  model <- ranger(
    formula = potability ~ .,
    data = train,
    num.trees = 500, 
    mtry = hyper_grid$mtry[i],
    min.node.size = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sample_size[i],
    seed = 123
  )
  hyper_grid$OOB_ERR[i] <- model$prediction.error #pastreaza eroarea pentru fiecare combinatie
}

hyper_grid %>%
  arrange(OOB_ERR) %>%
  top_n(-10)
```

Identificam parametrii optimi: mtry = 19 atribute, node_size = 3, sample_size = 0.8.

2. Model ajustat prin tuning:

```{r}
mod_rf_tuned <- randomForest(
  formula = potability ~ .,
  data = train,
  num.trees = 500,
  mtry  = 19,
  min.node.size = 3,
  sample.fraction = .8
)
mod_rf_tuned
```

```{r}
pred_mod_rf_tuned <- predict(mod_rf_tuned, test, target ="class")  
(cm_tuned <- confusionMatrix(pred_mod_rf_tuned, factor(test$potability)))
```


3. Model cu caret si Cross-Validation:

```{r}
mod_rf2 <- train(potability~.,
                data = train,
                method = 'rf',
                trControl = train_control)
mod_rf2
```
```{r}
pred_mod_rf2 <- predict(mod_rf2, test, target ="class")  
(cm2 <- confusionMatrix(pred_mod_rf2, factor(test$potability)))
```



  


## Evaluarea performantei modelelor:

```{r}
pred_rf1_roc <- predict(mod_rf1, test, type="prob")  
pred_rf2_roc <- predict(mod_rf2, test, type="prob")  
pred_rf_tuned <- predict(mod_rf_tuned, test, type = "prob")
```


Realizam graficul curbelor ROC pentru cele 3 modele RF:
```{r}
data_frame <- data.frame(actual.class = test$potability, probability = pred_rf1_roc, pred_rf2_roc, pred_rf_tuned)
roc_rf1 <- roc(actual.class~pred_rf1_roc[,1], data_frame)
roc_rf2 <- roc(actual.class~pred_rf2_roc[,1], data_frame)
roc_rf_tuned <- roc(actual.class~pred_rf_tuned[,1], data_frame)


df1 <- data.frame(
  specificity1 <- roc_rf1$specificities, 
  sensitivity1 <- roc_rf1$sensitivities)

df2 <- data.frame(
  specificity2 <- roc_rf2$specificities, 
  sensitivity2 <- roc_rf2$sensitivities)

df3 <- data.frame(
  specificity3 <- roc_rf_tuned$specificities, 
  sensitivity3 <- roc_rf_tuned$sensitivities)

ggplot() + 
  geom_line(df1, mapping = aes(x= specificity1, y = sensitivity1), color = "dark green") + 
  scale_x_reverse() +
  theme(text = element_text(size = 15))+
geom_line(df2, mapping = aes(x = specificity2, y = sensitivity2), color = "blue") + 
  scale_x_reverse() +
  theme(text = element_text(size = 15))+
geom_line(df3, mapping = aes(x = specificity3, y = sensitivity3), color = "orange") + 
  scale_x_reverse() +
  theme(text = element_text(size = 15))+
labs(
  title = 'ROC - Modele Random Forest',
  x = 'specificitate',
  y = 'senzitivitate')
```

Analizam performanta celor 3 modele:
```{r}
model <- c('rf1','rf2','rf_tuned')
auc <- c(auc(roc_rf1), auc(roc_rf2), auc(roc_rf_tuned))
accuracy <- c(cm1$overall['Accuracy'], cm2$overall['Accuracy'], cm_tuned$overall['Accuracy'])
sensitivity <- c(cm1$byClass['Sensitivity'], cm2$byClass['Sensitivity'], cm_tuned$byClass['Sensitivity'])
specificity <- c(cm1$byClass['Specificity'], cm2$byClass['Specificity'], cm_tuned$byClass['Specificity'])
performance <- data.frame(model, accuracy, auc, sensitivity, specificity)
performance
```

Vom alege modelul rf_2 drept cel mai bun Random Forest.

```{r}
best_rf <- mod_rf2
```










### Evaluare Curbe ROC 

```{r}
pred_lr_roc <- predict(best_lr, test, type = "prob")  
pred_nb_roc <- predict(best_nb, test, type = "prob")  
pred_tree_roc <- predict(best_tree, test, target = "class")
pred_bagged_roc <- predict(best_bagged, test, type = "prob")
pred_rf_roc <- predict(best_rf, test, type = "prob")


df_roc <- data.frame(actual.class = test$potability, probability = pred_lr_roc, pred_nb_roc, pred_tree_roc, pred_bagged_roc, pred_rf_roc)
roc_lr <- roc(actual.class~pred_lr_roc[,1], df_roc)
roc_nb <- roc(actual.class~pred_nb_roc$posterior[,1], df_roc)
roc_tree <- roc(actual.class~pred_tree_roc[,1], df_roc)
roc_bagged <- roc(actual.class~pred_bagged_roc[,1], df_roc)
roc_rf <- roc(actual.class~pred_rf_roc[,1], df_roc)

df_lr <- data.frame(
  specificity1 <- roc_lr$specificities, 
  sensitivity1 <- roc_lr$sensitivities)

df_nb <- data.frame(
  specificity2 <- roc_nb$specificities, 
  sensitivity2 <- roc_nb$sensitivities)

df_tree <- data.frame(
  specificity3 <- roc_tree$specificities, 
  sensitivity3 <- roc_tree$sensitivities)

df_bag <- data.frame(
  specificity4 <- roc_bagged$specificities, 
  sensitivity4 <- roc_bagged$sensitivities)

df_rf <- data.frame(
  specificity5 <- roc_rf$specificities, 
  sensitivity5 <- roc_rf$sensitivities)

colors <- c("Regresie Logistica" = "orange", "Naive Bayes" = "yellow", "Decision Tree" = "dark green", "Bagging" = "red", "Random Forest" = "blue")

ggplot() + 
  geom_line(df_lr, mapping = aes(x= specificity1, y = sensitivity1, color = "Regresie Logistica")) +
  scale_x_reverse() +
  theme(text = element_text(size = 15))+
geom_line(df_nb, mapping = aes(x = specificity2, y = sensitivity2, color = "Naive Bayes")) + 
  scale_x_reverse() +
  theme(text = element_text(size = 15))+
geom_line(df_tree, mapping = aes(x = specificity3, y = sensitivity3, color = "Decision Tree")) + 
  scale_x_reverse() +
  theme(text = element_text(size = 15))+
geom_line(df_bag, mapping = aes(x = specificity4, y = sensitivity4, color = "Bagging")) + 
  scale_x_reverse() +
  theme(text = element_text(size = 15))+
geom_line(df_rf, mapping = aes(x = specificity5, y = sensitivity5, color = "Random Forest")) + 
  scale_x_reverse() +
  theme(text = element_text(size = 15))+
labs(
  title = 'ROC - Modele  de clasificare',
  x = 'specificitate',
  y = 'senzitivitate',
  color = 'Model')+
  scale_color_manual(values = colors)
```



Matricele de confuzie pentru extragerea masurilor de evaluare a performantei:
```{r}
pred_lr_roc <- as_tibble(pred_lr_roc) %>% mutate(class = ifelse(No >= Yes, "No", "Yes"))
cm_lr <- confusionMatrix(factor(pred_lr_roc$class), factor(test$potability))

cm_nb <- confusionMatrix(pred_nb_roc$class, factor(test$potability))

pred_tree_roc <- as_tibble(pred_tree_roc) %>% mutate(class = ifelse(No >= Yes, "No", "Yes"))
cm_tree <- confusionMatrix(factor(pred_tree_roc$class), factor(test$potability))

pred_bagged_roc <- as_tibble(pred_bagged_roc) %>% mutate(class = ifelse(No >= Yes, "No", "Yes"))
cm_bag <- confusionMatrix(factor(pred_bagged_roc$class), factor(test$potability))
pred_rf_roc <- as_tibble(pred_rf_roc) %>% mutate(class = ifelse(No >= Yes, "No", "Yes"))
cm_rf <- confusionMatrix(factor(pred_rf_roc$class), factor(test$potability))

```



Tabelul cu parametrii de evaluare aferenti fiecarui model, ordonat dupa acuratețe si AUC:
```{r}
model <- c('lr','nb','tree','bagging','rf')
auc <- c(auc(roc_lr), auc(roc_nb), auc(roc_tree), auc(roc_bagged), auc(roc_rf))

accuracy <- c(cm_lr$overall['Accuracy'], cm_nb$overall['Accuracy'], cm_tree$overall['Accuracy'], cm_bag$overall['Accuracy'],cm_rf$overall['Accuracy'])

sensitivity <- c(cm_lr$byClass['Sensitivity'], cm_nb$byClass['Sensitivity'], cm_tree$byClass['Sensitivity'], cm_bag$byClass['Sensitivity'],cm_rf$byClass['Sensitivity'])

specificity <- c(cm_lr$byClass['Specificity'], cm_nb$byClass['Specificity'], cm_tree$byClass['Specificity'], cm_bag$byClass['Specificity'], cm_rf$byClass['Specificity'])

p_value <- c(cm_lr$overall['AccuracyPValue'], cm_nb$overall['AccuracyPValue'], cm_tree$overall['AccuracyPValue'], cm_bag$overall['AccuracyPValue'],cm_rf$overall['AccuracyPValue'])

performance <- data.frame(model, accuracy, auc, sensitivity, specificity, p_value)
performance %>% arrange(desc(accuracy), desc(auc))
```


În concluzie, cea mai bună predicție de clasificare se obține prin utilizarea modelului Random Forest. Ordinea performanței este cea din tabelul de mai sus.

