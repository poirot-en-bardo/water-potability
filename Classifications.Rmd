---
title: "R Notebook"
output: html_notebook
---


#SETUP

```{r}
library(tidyverse)
library(modelr)
library(caret)
library(rsample)
library(corrplot)
library(dplyr)
library(magrittr)
library(corrplot)
library(rpart)
library(rpart.plot)
library(ISLR)
library(partykit)
library(tree)
library(ipred)
library(randomForest)
library(ranger)
library(pROC)

water <- read.csv("water_quality.csv")
data(water)
water$ammonia <- as.numeric(water$ammonia)
(water <- drop_na(water) %>%
  rename(potability = is_safe))
water <- water %>% 
  mutate(potability = ifelse(potability == 0, "No", "Yes"))
water <- water %>% mutate(potability = factor(potability))
```

```{r}
water <- drop_na(water)
```


```{r}
set.seed(123)
split <- initial_split(water, prop=0.7, strata= "potability")
train <-training(split)
test <- testing(split)

#observăm că se mentine proportia
table(water$potability) #11.4%
table(train$potability)
table(test$potability)
```


Selectam toate atributele in afara de potability
```{r}
features <- setdiff(names(train), "potability")
features
```



------------- Naive Bayes ------------------


Separam variabilele
```{r}
#x sunt variabilele independente (predictori)
#y este variabila dependenta (potabilitatea)
x <- train[,features]
y <- train$potability
```


10-fold Cross-Validation
```{r}
train_control <- trainControl(
  method="cv",
  number=10) 
```




Primul model
Observăm că avem probabilități nule, ceea ce înseamnă că va trebui să adăugăm Laplace Smoothing:
```{r}
mod_nb1 <- train(
  x = x,
  y = y,
  method = "nb",
  trControl = train_control )

mod_nb1
```
```{r}
confusionMatrix(mod_nb1)
```


Search grid pentru a identifica parametrii optimi:
```{r}
search_grid <- expand.grid(
  usekernel = c(TRUE, FALSE),
  fL = 0.5, #la variabilele nominale sa nu porneasca de la 0
  adjust = seq(0, 5, by = 1) ) #ajusteaza kernelii cu (1,2,3,4,5)

mod_nb2 = train(
  x = x,
  y = y,
  method = "nb",
  trControl = train_control,
  tuneGrid = search_grid     
)

mod_nb2
```



```{r}
confusionMatrix(mod_nb2)
```

Observăm o acuratețe mai mare (90,83% vs 85,6%), deci alegem acest model.
```{r}
finalModel <- mod_nb2$finalModel
```



```{r}
#predictie pe setul de test, cu probabilitatea aferenta
pred <- predict(finalModel, test, type="prob")
#compara predictiile cu valorile reale
confusionMatrix(pred$class, test$potability)
#95% CI = confidence intervals
#no information rate (NIR) e sansa de a face clasificarea random, fara un model
```









__________________________________________________________________________________________

## Arbori de decizie 


1. Arbore simplu folosind pachetul rpart:
```{r}
set.seed(123)
mod_tree1 = rpart(
  formula = potability ~. ,  
  data = train,
  method = "class" 
)
mod_tree1
#nr instante, loss = nr erori (clasificate gresit pe clasa majoritara), clasa majoritara, probabilitatile predictiei 
```

```{r}
summary(mod_tree1)
#CP = cost complexity parameters
#penalizeaza arborele pe baza nr de frunze
```


```{r}
rpart.plot(mod_tree1)
```


```{r}
#predictie pe test
pred_mod_tree1 <- predict(mod_tree1, newdata = test, target ="class")
#adaugam coloana cu no/yes
pred_mod_tree1 <- as_tibble(pred_mod_tree1) %>% mutate(class = ifelse(No >= Yes, "No", "Yes"))
(cmt1 <- confusionMatrix(factor(pred_mod_tree1$class), factor(test$potability)))
```


Comparatie cu un arbore simplu realizat cu Cross-Validation:
```{r}
mod_tree_cv <- train(x,y,
                     method = "rpart",
                      trControl = train_control)
pred_mod_tree_cv <- predict(mod_tree_cv, newdata = test, target ="class")
(cmt_cv <- confusionMatrix(pred_mod_tree_cv, factor(test$potability)))
```
Modelul realizat prin rpart este cu putin mai bun.





2. Arbore netaiat
```{r}
mod_tree_full <- rpart(potability ~., 
            data = train,
            method = "class",
            control = list(cp=0)) 
rpart.plot(mod_tree_full)
```


Acuratete mai buna pentru arborele netaiat:
```{r}
pred_m2 <- predict(m2, newdata = test, target = "class")
pred_m2 <- as_tibble(pred_m2) %>% mutate(class = ifelse(No >= Yes, "No", "Yes"))
(cmt2 <- confusionMatrix(factor(pred_m2$class), factor(test$potability)))
```


3. Pruning: 

```{r}
m2_pruned <- prune(m2, cp = 0.005)
pred_m2_pruned <- predict(m2_pruned, newdata = test, target = "class")
pred_m2_pruned <- as_tibble(pred_m2_pruned) %>% mutate(class = ifelse(No >= Yes, "No", "Yes"))
(cmt3 <- confusionMatrix(factor(pred_m2_pruned$class), factor(test$potability)))

```



4. Cu optimizarea entropiei, folosind functia tree():
```{r}
set.seed(123)
mod_tree_entropy <- tree(potability ~., data = train) 
summary(mod_tree_entropy)
```


```{r}
pred_tree_entropy <- predict(mod_tree_entropy, newdata = test, target = "class")
pred_tree_entropy <- as_tibble(pred_tree_entropy) %>% mutate(class = ifelse(No >= Yes, "No", "Yes"))
(cmt4 <- confusionMatrix(factor(pred_tree_entropy$class), factor(test$potability)))
```


5. Cu optimizarea coeficientului Gini

```{r}
set.seed(123)
mod_tree_gini <- tree(potability ~., data = train, split="gini")
summary(mod_tree_gini)
```


```{r}
pred_tree_gini <- predict(mod_tree_gini, newdata = test, target = "class")
pred_tree_gini <- as_tibble(pred_tree_gini) %>% mutate(class = ifelse(No >= Yes, "No", "Yes"))
(cmt5 <- confusionMatrix(factor(pred_tree_gini$class), factor(test$potability)))
```




Evaluarea performantei arborilor de decizie:


```{r}
model <- c('tree1','tree_full','tree_pruned','tree_entropy','tree_gini')
accuracy <- c(cmt1$overall['Accuracy'], cmt2$overall['Accuracy'], cmt3$overall['Accuracy'],cmt4$overall['Accuracy'],cmt5$overall['Accuracy'])
sensitivity <- c(cmt1$byClass['Sensitivity'], cmt2$byClass['Sensitivity'], cmt3$byClass['Sensitivity'],cmt4$byClass['Sensitivity'],cmt5$byClass['Sensitivity'])
specificity <- c(cmt1$byClass['Specificity'], cmt2$byClass['Specificity'], cmt3$byClass['Specificity'],cmt4$byClass['Specificity'],cmt4$byClass['Specificity'])
performance <- data.frame(model, accuracy, sensitivity, specificity)
performance
```

În acest caz, desi arborele netaiat pare cel mai perfomant, vom face exceptia de alege următorul model ca performanță (tree_gini), pentru a evita overfitting-ul.

```{r}
best_tree <- mod_tree_gini
```





------------------------- Bagging ------------------------


1. Model simplu:
```{r}
set.seed(123)
mod_bagged1 <- bagging(potability ~ .,
                     data = train, coob = TRUE)
mod_bagged1
```
Acuratetea de training = 100 - 3.82 = 96.18%


```{r}
pred_mod_bagged1 <- predict(mod_bagged1, newdata = test, target = "class")
(cmb1 <- confusionMatrix(pred_mod_bagged1, factor(test$potability)))
```


Grafic pentru stabilizarea erorii de clasificare:
```{r}
ntree <- seq(10, 50, by = 1)
misclassification <- vector(mode = "numeric", length = length(ntree))
for (i in seq_along(ntree)) {
  set.seed(123)
  model <- bagging( 
    potability ~.,
    data = train,
    coob = TRUE,
    nbag = ntree[i])
  misclassification[i] = model$err
}

plot(ntree, misclassification, type="l", lwd="2")
```

2. Model cu optimizarea numarului de bags:
Optăm pentru 36 de bags:

```{r}
mod_bagged_36 <- bagging(potability ~ .,
                     data = train, coob = TRUE, nbag = 36)
mod_bagged_36
```

```{r}
pred_mod_bagged_36 <- predict(mod_bagged_36, newdata = test, target = "class")
(cmb2 <- confusionMatrix(pred_mod_bagged_36, factor(test$potability)))
```



3. Model Bagging cu caret si Cross-Validation:

```{r}
mod_bagged_cv <- train(x,
                   y,
                   method="treebag",
                   trControl=train_control,
                   nbag = 36,
                   importance=TRUE)

mod_bagged_cv
```


```{r}
pred_mod_bagged_cv <- predict(mod_bagged_cv, newdata = test, target = "class")
(cmb3 <- confusionMatrix(pred_mod_bagged_cv, factor(test$potability)))
```



```{r}
model <- c('bagged1','bagged2','bagged_cv')
accuracy <- c(cmb1$overall['Accuracy'], cmb2$overall['Accuracy'], cmb3$overall['Accuracy'])
sensitivity <- c(cmb1$byClass['Sensitivity'], cmb2$byClass['Sensitivity'], cmb3$byClass['Sensitivity'])
specificity <- c(cmb1$byClass['Specificity'], cmb2$byClass['Specificity'], cmb3$byClass['Specificity'])
performance <- data.frame(model, accuracy, sensitivity, specificity)
performance
```

Vom selecta modelul bagged2 drept cel mai performant:

```{r}
bagged_best <- mod_bagged_36
```


---------------- Random Forest ----------------------------



1. Model simplu cu biblioteca randomForest:
```{r}
set.seed(123)
mod_rf1 <- randomForest(
  formula = potability ~ .,
  data = train
) #selecteaza aleatoriu 6 variabile la fiecare impartire 
#pt ca avem 20 variabile in total si m=p/3 =11/3 aprox = 3
mod_rf1
```


Vizualizarea evolutiei ratei erorii de clasificare:
```{r}
plot(mod_rf1)
```


```{r}
pred_mod_rf1 <- predict(mod_rf1, test, target ="class")  
(cm1 <- confusionMatrix(pred_mod_rf1, factor(test$potability)))
```


Tuning:

```{r}
set.seed(123)
hyper_grid <- expand.grid(
  mtry = seq(2, 20, by = 1), # nr de atribute dintr-un split;
  node_size = seq(3, 9, by = 2), 
  sample_size = c(.55, .632, .7, .8),
  OOB_ERR = 0 
)


for (i in 1:nrow(hyper_grid)) {
  model <- ranger(
    formula = potability ~ .,
    data = train,
    num.trees = 500, 
    mtry = hyper_grid$mtry[i],
    min.node.size = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sample_size[i],
    seed = 123
  )
  hyper_grid$OOB_ERR[i] <- model$prediction.error #pastreaza eroarea pentru fiecare combinatie
}

hyper_grid %>%
  arrange(OOB_ERR) %>%
  top_n(-10)
```

Identificam parametrii optimi: mtry = 19 atribute, node_size = 3, sample_size = 0.8.

2. Model ajustat prin tuning:

```{r}
mod_rf_tuned <- randomForest(
  formula = potability ~ .,
  data = train,
  num.trees = 500,
  mtry  = 19,
  min.node.size = 3,
  sample.fraction = .8
)
mod_rf_tuned
```

```{r}
pred_mod_rf_tuned <- predict(mod_rf_tuned, test, target ="class")  
(cm_tuned <- confusionMatrix(pred_mod_rf_tuned, factor(test$potability)))
```


3. Model cu caret si Cross-Validation:

```{r}
mod_rf2 <- train(potability~.,
                data = train,
                method = 'rf',
                trControl = train_control)
mod_rf2
```
```{r}
pred_mod_rf2 <- predict(mod_rf2, test, target ="class")  
(cm2 <- confusionMatrix(pred_mod_rf2, factor(test$potability)))
```






### Evaluarea performantei modelelor:

```{r}
pred_rf1_roc <- predict(mod_rf1, test, type="prob")  
pred_rf2_roc <- predict(mod_rf2, test, type="prob")  
pred_rf_tuned <- predict(mod_rf_tuned, test, type = "prob")
```


Realizam graficul curbelor ROC pentru cele 3 modele:
```{r}
dataset <- data.frame(actual.class = test$potability, probability = pred_rf1_roc, pred_rf2_roc, pred_rf_tuned)
roc_rf1 <- roc(actual.class~pred_rf1_roc[,1], dataset)
roc_rf2 <- roc(actual.class~pred_rf2_roc[,1], dataset)
roc_rf_tuned <- roc(actual.class~pred_rf_tuned[,1], dataset)


df1 <- data.frame(
  specificity1 <- roc_rf1$specificities, 
  sensitivity1 <- roc_rf1$sensitivities)

df2 <- data.frame(
  specificity2 <- roc_rf2$specificities, 
  sensitivity2 <- roc_rf2$sensitivities)

df3 <- data.frame(
  specificity3 <- roc_rf_tuned$specificities, 
  sensitivity3 <- roc_rf_tuned$sensitivities)

ggplot() + 
  geom_line(df1, mapping = aes(x= specificity1, y = sensitivity1), color = "dark green") + 
  scale_x_reverse() +
  theme(text = element_text(size = 20))+
geom_line(df2, mapping = aes(x = specificity2, y = sensitivity2), color = "blue") + 
  scale_x_reverse() +
  theme(text = element_text(size = 20))+
geom_line(df3, mapping = aes(x = specificity3, y = sensitivity3), color = "orange") + 
  scale_x_reverse() +
  theme(text = element_text(size = 20))+
labs(
  title = 'ROC - Modele Random Forest',
  x = 'specificitate',
  y = 'senzitivitate')
```

Analizam performanta celor 3 modele:
```{r}
model <- c('rf1','rf2','rf_tuned')
auc <- c(auc(roc_rf1), auc(roc_rf2), auc(roc_rf_tuned))
accuracy <- c(cm1$overall['Accuracy'], cm2$overall['Accuracy'], cm_tuned$overall['Accuracy'])
sensitivity <- c(cm1$byClass['Sensitivity'], cm2$byClass['Sensitivity'], cm_tuned$byClass['Sensitivity'])
specificity <- c(cm1$byClass['Specificity'], cm2$byClass['Specificity'], cm_tuned$byClass['Specificity'])
performance <- data.frame(model, accuracy, auc, sensitivity, specificity)
performance
```

Vom alege modelul rf_tuned drept cel mai bun Random Forest.

```{r}
rf_best <- mod_rf_tuned
```











------------------- Evaluare Curbe ROC -----------------------

```{r}
summary(pred_mod_bagged1)
```
